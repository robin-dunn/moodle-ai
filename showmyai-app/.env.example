# Server Configuration
NODE_ENV=production
PORT=3000

# Logging
LOG_LEVEL=info

# AI Provider Configuration
# Options: 'ollama' (default, local LLM), 'gemini', or 'openai'
AI_PROVIDER=ollama

# Ollama Configuration (Default Provider - Local LLM)
OLLAMA_BASE_URL=http://ollama:11434
OLLAMA_MODEL=phi3
OLLAMA_TEMPERATURE=0.7

# Google Gemini Configuration (Alternative Provider)
GEMINI_API_KEY=your_gemini_api_key_here
GEMINI_MODEL=gemini-1.5-flash
GEMINI_TEMPERATURE=0.7
GEMINI_MAX_TOKENS=1000

# OpenAI Configuration (Alternative Provider)
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-3.5-turbo
OPENAI_TEMPERATURE=0.7
OPENAI_MAX_TOKENS=1000

# Chat Configuration
CHAT_SESSION_TIMEOUT=86400000
CHAT_RATE_LIMIT_WINDOW=3600000
CHAT_RATE_LIMIT_MAX_REQUESTS=100

# Future: Moodle API Configuration (optional)
# MOODLE_API_URL=http://moodle:8080
# MOODLE_API_TOKEN=your_api_token_here
